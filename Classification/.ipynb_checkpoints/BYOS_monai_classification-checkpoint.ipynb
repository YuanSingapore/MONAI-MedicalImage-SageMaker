{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker DICOM Training Overview\n",
    "\n",
    "In this example we will demonstrate how to integrate the [MONAI](http://monai.io) framework into Amazon SageMaker, and give example code of MONAI pre-processing transforms and neural network (DenseNet) that you can use to train a medical image classification model using DICOM images directly.  \n",
    "\n",
    "Please also visit [Build a medical image analysis pipeline on Amazon SageMaker using the MONAI framework](https://aws.amazon.com/blogs/industries/build-a-medical-image-analysis-pipeline-on-amazon-sagemaker-using-the-monai-framework/) for additional details on how to deploy the MONAI model, pipe input data from S3, and perform batch inferences using SageMaker batch transform.\n",
    "\n",
    "For more information about the PyTorch in SageMaker, please visit [sagemaker-pytorch-containers](https://github.com/aws/sagemaker-pytorch-containers) and [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk) github repositories.\n",
    "\n",
    "Sample dataset is obtained from this [source COVID-CT-MD](https://github.com/ShahinSHH/COVID-CT-MD). The total dataset contains volumetric chest CT scans (DICOM files) of 169 patients positive for COVID-19 infection, 60 patients with CAP (Community Acquired Pneumonia), and 76 normal patients. For this demo purpose, only 26 images are randomly selected. The selection and preprocessing are not included in this demo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install necessary libries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/MONAI-MedicalImage-SageMaker/Classification'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: monai in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from -r ./code/requirements.txt (line 1)) (0.8.0)\n",
      "Requirement already satisfied: itk in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from -r ./code/requirements.txt (line 2)) (5.2.1.post1)\n",
      "Requirement already satisfied: pillow in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from -r ./code/requirements.txt (line 3)) (8.4.0)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from -r ./code/requirements.txt (line 4)) (1.1.5)\n",
      "Requirement already satisfied: torch>=1.6 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from monai->-r ./code/requirements.txt (line 1)) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from monai->-r ./code/requirements.txt (line 1)) (1.19.2)\n",
      "Requirement already satisfied: itk-filtering==5.2.1.post1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from itk->-r ./code/requirements.txt (line 2)) (5.2.1.post1)\n",
      "Requirement already satisfied: itk-registration==5.2.1.post1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from itk->-r ./code/requirements.txt (line 2)) (5.2.1.post1)\n",
      "Requirement already satisfied: itk-numerics==5.2.1.post1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from itk->-r ./code/requirements.txt (line 2)) (5.2.1.post1)\n",
      "Requirement already satisfied: itk-core==5.2.1.post1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from itk->-r ./code/requirements.txt (line 2)) (5.2.1.post1)\n",
      "Requirement already satisfied: itk-io==5.2.1.post1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from itk->-r ./code/requirements.txt (line 2)) (5.2.1.post1)\n",
      "Requirement already satisfied: itk-segmentation==5.2.1.post1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from itk->-r ./code/requirements.txt (line 2)) (5.2.1.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pandas->-r ./code/requirements.txt (line 4)) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pandas->-r ./code/requirements.txt (line 4)) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas->-r ./code/requirements.txt (line 4)) (1.15.0)\n",
      "Requirement already satisfied: typing_extensions in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torch>=1.6->monai->-r ./code/requirements.txt (line 1)) (4.0.1)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torch>=1.6->monai->-r ./code/requirements.txt (line 1)) (0.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ./code/requirements.txt\n",
    "!mkdir -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (1.10.1)\n",
      "Collecting torch\n",
      "  Using cached torch-1.10.2-cp36-cp36m-manylinux1_x86_64.whl (881.9 MB)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (0.11.2)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torch) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torch) (4.0.1)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torchvision) (1.19.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch torchvision  ## upgrade torchvision to ensure consistent performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket: sagemaker-us-east-1-741261399688\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import sagemaker \n",
    "\n",
    "sess = sagemaker.Session()\n",
    "env_path = Path('.') / 'set.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "bucket=sess.default_bucket() ## replace with <your bucket for the dataset>\n",
    "bucket_path=os.environ.get('BUCKET_PATH')\n",
    "user=os.environ.get('DICOM_USER')\n",
    "password = os.environ.get('DICOM_PASSWORD')\n",
    "datadir = 'data'\n",
    "print('Bucket: '+bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload training dataset in S3\n",
    "\n",
    "for this demo, we only use 25 images for model training\n",
    "I have already downloaded the image, save in data folder \n",
    "\n",
    "+ *.dcm are the dicome images\n",
    "+ manifest.json stores labels for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file_list=os.listdir(datadir)\n",
    "\n",
    "\n",
    "image_file_list = [x  for x in image_file_list if x.endswith('dcm') ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the dataset and display them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MLflow support for Python 3.6 is deprecated and will be dropped in an upcoming release. At that point, existing Python 3.6 workflows that use MLflow will continue to work without modification, but Python 3.6 users will no longer get access to the latest MLflow features and bugfixes. We recommend that you upgrade to Python 3.7 or newer.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'image_file_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0a9d082ab2de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m ])\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mimage_file_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;31m## check image size after preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_file_list' is not defined"
     ]
    }
   ],
   "source": [
    "import monai\n",
    "from monai.transforms import Compose, LoadImage, Resize, ScaleIntensity, ToTensor, SqueezeDim, RandRotate,RandFlip,RandZoom\n",
    "import matplotlib.pyplot as plt\n",
    "# define transform functions \n",
    "## preprocess the dataset before trainining using MONAI.  Based on img.shape, this is a channel last image\n",
    "train_transforms = Compose([\n",
    "    LoadImage(image_only=True),\n",
    "    ScaleIntensity(),\n",
    "    RandRotate(range_x=15, prob=0.5, keep_size=True),\n",
    "    RandFlip(spatial_axis=0, prob=0.5),\n",
    "    #RandZoom(min_zoom=0.9, max_zoom=1.1, prob=0.5, keep_size=True),\n",
    "    Resize(spatial_size=(512,-1)),\n",
    "    ToTensor()\n",
    "])\n",
    "img = train_transforms(datadir+'/'+image_file_list[0])\n",
    "img.shape ## check image size after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display sample of DICOM Images\n",
    "inf_test = []\n",
    "inf_test_label = []\n",
    "\n",
    "trans = Compose([LoadImage(image_only=True), Resize(spatial_size=(512,-1))])\n",
    "plt.subplots(2, 2, figsize=(8, 8))\n",
    "for i in range(0,4):\n",
    "    #s3.download_file(bucket, image_file_list[i], datadir+'/'+image_file_list[i])\n",
    "    \n",
    "    img = trans(datadir+'/'+image_file_list[i])\n",
    "    print(img.shape)\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.xlabel(image_file_list[i])\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title(image_file_list[i].split('-')[0])\n",
    "    inf_test.append(datadir+'/'+image_file_list[i])\n",
    "    inf_test_label.append(image_file_list[i].split('-')[0])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['normal-IM0062.dcm', 'cap-IM0032.dcm', 'covid-IM0073.dcm', 'cap-IM0064.dcm']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadir_test='test_data'\n",
    "\n",
    "image_file_list=os.listdir(datadir_test)\n",
    "image_file_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "### Create Sagemaker session and S3 location for DICOM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'upload_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-8dbb6fd05d27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m## IF UPLOAD THE DATA TO S3, DO THE FOLLOWING STEP. we may skip the step if the the data has already been uploaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input spec as an S3 path: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'upload_data'"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "import os\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "#inputs='s3://dataset-pathology/CovidTrainingV2'\n",
    "\n",
    "sess =sess.default_bucket()\n",
    "\n",
    "key='CovidTraining'\n",
    "path=os.path.join(\"s3://\",bucket,key)\n",
    "#S3Uploader.upload('./data', path) \n",
    "\n",
    "\n",
    "## IF UPLOAD THE DATA TO S3, DO THE FOLLOWING STEP. we may skip the step if the the data has already been uploaded\n",
    "inputs = sess.upload_data(path=datadir, bucket=bucket,key_prefix=key)\n",
    "\n",
    "print('input spec as an S3 path: {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "### Training\n",
    "\n",
    "The ```monai_dicom.py``` script provides all the code we need for training and hosting a SageMaker model (model_fn function to load a model). The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, such as:\n",
    "\n",
    "* SM_MODEL_DIR: A string representing the path to the directory to write model artifacts to. These artifacts are uploaded to S3 for model hosting.\n",
    "* SM_NUM_GPUS: The number of gpus available in the current container.\n",
    "* SM_CURRENT_HOST: The name of the current container on the container network.\n",
    "* SM_HOSTS: JSON encoded list containing all the hosts .\n",
    "Supposing one input channel, 'training', was used in the call to the PyTorch estimator's fit() method, the following will be set, following the format SM_CHANNEL_[channel_name]:\n",
    "\n",
    "* SM_CHANNEL_TRAINING: A string representing the path to the directory containing data in the 'training' channel.\n",
    "For more information about training environment variables, please visit [SageMaker Containers](https://github.com/aws/sagemaker-containers).\n",
    "\n",
    "A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to model_dir so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an argparse.ArgumentParser instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001b[39;49;00m\n",
      "\u001b[37m# SPDX-License-Identifier: MIT-0\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdistributed\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mdist\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctional\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mF\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdistributed\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Dataset, DataLoader\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mPIL\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Image\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmonai\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mconfig\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_config\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmonai\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtransforms\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m \\\n",
      "    Compose, LoadImage, Resize, ScaleIntensity, ToTensor, RandRotate, RandFlip, RandZoom\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmonai\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnetworks\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m densenet121\n",
      "\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
      "logger.setLevel(logging.DEBUG)\n",
      "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
      "\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mDICOMDataset\u001b[39;49;00m(Dataset):\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, image_files, labels, transforms):\n",
      "        \u001b[36mself\u001b[39;49;00m.image_files = image_files\n",
      "        \u001b[36mself\u001b[39;49;00m.labels = labels\n",
      "        \u001b[36mself\u001b[39;49;00m.transforms = transforms\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__len__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.image_files)\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__getitem__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, index):\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.transforms(\u001b[36mself\u001b[39;49;00m.image_files[index]), \u001b[36mself\u001b[39;49;00m.labels[index]\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_train_data_loader\u001b[39;49;00m(batch_size, trainX, trainY, is_distributed, **kwargs):\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet train data loader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    train_transforms = Compose([\n",
      "        LoadImage(image_only=\u001b[34mTrue\u001b[39;49;00m),\n",
      "        ScaleIntensity(),\n",
      "        RandRotate(range_x=\u001b[34m15\u001b[39;49;00m, prob=\u001b[34m0.5\u001b[39;49;00m, keep_size=\u001b[34mTrue\u001b[39;49;00m),\n",
      "        RandFlip(spatial_axis=\u001b[34m0\u001b[39;49;00m, prob=\u001b[34m0.5\u001b[39;49;00m),\n",
      "       \u001b[37m# RandZoom(min_zoom=0.9, max_zoom=1.1, prob=0.5, keep_size=True),\u001b[39;49;00m\n",
      "        Resize(spatial_size=(\u001b[34m512\u001b[39;49;00m,-\u001b[34m1\u001b[39;49;00m)),\n",
      "        ToTensor()\n",
      "    ])\n",
      "    \n",
      "   \n",
      "    \n",
      "    \n",
      "    dataset = DICOMDataset(trainX, trainY, train_transforms)\n",
      "    \n",
      "    train_sampler = torch.utils.data.distributed.DistributedSampler(dataset) \u001b[34mif\u001b[39;49;00m is_distributed \u001b[34melse\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=train_sampler \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m,\n",
      "                                       sampler=train_sampler, **kwargs)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(args):\n",
      "    is_distributed = \u001b[36mlen\u001b[39;49;00m(args.hosts) > \u001b[34m1\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m args.backend \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mDistributed training - \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(is_distributed))\n",
      "    use_cuda = args.num_gpus > \u001b[34m0\u001b[39;49;00m\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mNumber of gpus available - \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.num_gpus))\n",
      "    kwargs = {\u001b[33m'\u001b[39;49;00m\u001b[33mnum_workers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[34m10\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpin_memory\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[34mTrue\u001b[39;49;00m} \u001b[34mif\u001b[39;49;00m use_cuda \u001b[34melse\u001b[39;49;00m {}\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m use_cuda \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \n",
      "    \u001b[34mif\u001b[39;49;00m is_distributed:\n",
      "        \u001b[37m# Initialize the distributed environment.\u001b[39;49;00m\n",
      "        world_size = \u001b[36mlen\u001b[39;49;00m(args.hosts)\n",
      "        os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mWORLD_SIZE\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = \u001b[36mstr\u001b[39;49;00m(world_size)\n",
      "        host_rank = args.hosts.index(args.current_host)\n",
      "        os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mRANK\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = \u001b[36mstr\u001b[39;49;00m(host_rank)\n",
      "        dist.init_process_group(backend=args.backend, rank=host_rank, world_size=world_size)\n",
      "        logger.debug(\u001b[33m'\u001b[39;49;00m\u001b[33mInitialized the distributed environment: \u001b[39;49;00m\u001b[33m\\'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\\'\u001b[39;49;00m\u001b[33m backend on \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m nodes. \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\n",
      "            args.backend, dist.get_world_size()) + \u001b[33m'\u001b[39;49;00m\u001b[33mCurrent host rank is \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m. Number of gpus: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\n",
      "            dist.get_rank(), args.num_gpus))\n",
      "\n",
      "    \u001b[37m# set the seed for generating random numbers\u001b[39;49;00m\n",
      "    torch.manual_seed(args.seed)\n",
      "    \u001b[34mif\u001b[39;49;00m use_cuda:\n",
      "        torch.cuda.manual_seed(args.seed)\n",
      "        \n",
      "    \u001b[37m#build file lists\u001b[39;49;00m\n",
      "    image_label_list = []\n",
      "    image_file_list = []\n",
      "    metadata = args.data_dir+\u001b[33m'\u001b[39;49;00m\u001b[33m/manifest.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m   \n",
      "    \u001b[37m# Load Labels\u001b[39;49;00m\n",
      "    \u001b[37m# open json files\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(metadata) \u001b[34mas\u001b[39;49;00m f:\n",
      "        manifest = json.load(f)\n",
      "    \n",
      "    \n",
      "    my_dictionary = {\u001b[33m'\u001b[39;49;00m\u001b[33mcap\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[34m1\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mnormal\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[34m0\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mcovid\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[34m2\u001b[39;49;00m}\n",
      "    class_names = \u001b[36mlist\u001b[39;49;00m(my_dictionary.keys())\n",
      "    num_class = \u001b[36mlen\u001b[39;49;00m(class_names)\n",
      "    \n",
      "    \u001b[37m# generate file list and label list\u001b[39;49;00m\n",
      "    image_file_list=[]\n",
      "    image_label_list=[]\n",
      "    \u001b[37m#class_names = 'Cardiomegaly'\u001b[39;49;00m\n",
      "    \n",
      "    image_label_list=[]\n",
      "    image_file_list=[]\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m manifest:\n",
      "            name = file[\u001b[33m'\u001b[39;49;00m\u001b[33mfilename\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "            filename = args.data_dir+\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m+name\n",
      "            image_file_list.append(filename)\n",
      "            label=file[\u001b[33m'\u001b[39;49;00m\u001b[33mcontent\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "            label_numeric=my_dictionary[label]\n",
      "            image_label_list.extend([[label_numeric]])\n",
      "    \u001b[37m#print('image_label_list ---', image_label_list)\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mTraining count =\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mlen\u001b[39;49;00m(image_file_list))\n",
      "            \n",
      "    train_loader = _get_train_data_loader(args.batch_size, image_file_list, image_label_list, \u001b[34mFalse\u001b[39;49;00m, **kwargs)\n",
      "\n",
      "    \u001b[37m#create model\u001b[39;49;00m\n",
      "    model = densenet121(\n",
      "        spatial_dims=\u001b[34m2\u001b[39;49;00m,\n",
      "        in_channels=\u001b[34m1\u001b[39;49;00m,\n",
      "        out_channels=\u001b[34m3\u001b[39;49;00m\n",
      "    ).to(device)\n",
      "    loss_function = torch.nn.CrossEntropyLoss()\n",
      "    optimizer = torch.optim.Adam(model.parameters(), \u001b[34m1e-5\u001b[39;49;00m)\n",
      "    epoch_num = args.epochs\n",
      "    val_interval = \u001b[34m1\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m#train model\u001b[39;49;00m\n",
      "    best_metric = -\u001b[34m1\u001b[39;49;00m\n",
      "    best_metric_epoch = -\u001b[34m1\u001b[39;49;00m\n",
      "    epoch_loss_values = \u001b[36mlist\u001b[39;49;00m()\n",
      "    metric_values = \u001b[36mlist\u001b[39;49;00m()\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(epoch_num):\n",
      "        logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m * \u001b[34m10\u001b[39;49;00m)\n",
      "        logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mepoch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch + \u001b[34m1\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch_num\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        model.train()\n",
      "        epoch_loss = \u001b[34m0\u001b[39;49;00m\n",
      "        step = \u001b[34m0\u001b[39;49;00m\n",
      "        \u001b[34mfor\u001b[39;49;00m batch_data \u001b[35min\u001b[39;49;00m train_loader:\n",
      "           \n",
      "            step += \u001b[34m1\u001b[39;49;00m\n",
      "            \u001b[37m#inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\u001b[39;49;00m\n",
      "            inputs = batch_data[\u001b[34m0\u001b[39;49;00m].to(device)\n",
      "            \n",
      "            \u001b[37m#logger.info('label type: ', batch_data[1].type())\u001b[39;49;00m\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33minputs shape is -----\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,inputs.shape)\n",
      "            \u001b[37m#inputs = inputs.to(memory_format=torch.channels_last) ######### debug here\u001b[39;49;00m\n",
      "            inputs = inputs.permute(\u001b[34m0\u001b[39;49;00m,\u001b[34m3\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m)\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33minputs shape after is -----\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,inputs.shape)\n",
      "\n",
      "            \u001b[37m# print('label from batch is',batch_data[1][0].to(device))  \u001b[39;49;00m\n",
      "            \u001b[37m#x=[[el] for el in batch_data[1]]\u001b[39;49;00m\n",
      "            \u001b[37m#label  = torch.tensor(batch_data[1])\u001b[39;49;00m\n",
      "            labels = batch_data[\u001b[34m1\u001b[39;49;00m][\u001b[34m0\u001b[39;49;00m].to(device)\n",
      "            optimizer.zero_grad()\n",
      "            outputs = model(inputs)\n",
      "            loss = loss_function(outputs, labels)\n",
      "            loss.backward()\n",
      "            optimizer.step()\n",
      "            epoch_loss += loss.item()\n",
      "            logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mstep\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mlen\u001b[39;49;00m(train_loader.dataset) // train_loader.batch_size\u001b[33m}\u001b[39;49;00m\u001b[33m, train_loss: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mloss.item()\u001b[33m:\u001b[39;49;00m\u001b[33m.4f\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "            epoch_len = \u001b[36mlen\u001b[39;49;00m(train_loader.dataset) // train_loader.batch_size        \n",
      "        epoch_loss /= step\n",
      "        epoch_loss_values.append(epoch_loss)\n",
      "        logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mepoch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch + \u001b[34m1\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m average loss: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch_loss\u001b[33m:\u001b[39;49;00m\u001b[33m.4f\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    save_model(model, args.model_dir)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msave_model\u001b[39;49;00m(model, model_dir):\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving the model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[37m# recommended way from http://pytorch.org/docs/master/notes/serialization.html\u001b[39;49;00m\n",
      "    torch.save(model.cpu().state_dict(), path)\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    model = densenet121(\n",
      "        spatial_dims=\u001b[34m2\u001b[39;49;00m,\n",
      "        in_channels=\u001b[34m1\u001b[39;49;00m,\n",
      "        out_channels=\u001b[34m3\u001b[39;49;00m\n",
      "    )\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model.load_state_dict(torch.load(f))\n",
      "    \u001b[34mreturn\u001b[39;49;00m model.to(device)   \n",
      "    \n",
      "    \n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    parser = argparse.ArgumentParser()\n",
      "    \n",
      "    \u001b[37m# Data and model checkpoints directories\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m100\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for training (default: 1000)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test-batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m100\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for testing (default: 100)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m5\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 5)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.01\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mlearning rate (default: 0.01)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--momentum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.5\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mSGD momentum (default: 0.5)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mrandom seed (default: 1)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--log-interval\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m100\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mhow many batches to wait before logging training status\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--backend\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34mNone\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mbackend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Container environment\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num-gpus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\n",
      "    train(parser.parse_args())\n"
     ]
    }
   ],
   "source": [
    "!pygmentize source/monai_dicom_json.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training in SageMaker\n",
    "\n",
    "The `PyTorch` class allows us to run our training function as a training job on SageMaker infrastructure.  We need to configure it with our training script, an IAM role, the number of training instances, the training instance type, and hyperparameters.  In this case we are going to run our training job on ```ml.m5.2xlarge``` instance.  But this example can be ran on one or multiple, cpu or gpu instances ([full list of available instances](https://aws.amazon.com/sagemaker/pricing/instance-types/)).  The hyperparameters parameter is a dict of values that will be passed to your training script -- you can see how to access these values in the ```monai_dicom.py``` script above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point='train.py',\n",
    "                    source_dir='code',\n",
    "                    role=role,\n",
    "                    framework_version='1.5.0',\n",
    "                    py_version='py3',\n",
    "                    instance_count=1,\n",
    "                    instance_type='ml.m5.2xlarge',\n",
    "                    hyperparameters={\n",
    "                        'backend': 'gloo',\n",
    "                        'epochs': 100\n",
    "                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've constructed our PyTorch object, we can fit it using the DICOM dataset we uploaded to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 9.06 µs\n",
      "2022-04-22 12:50:12 Starting - Starting the training job...ProfilerReport-1650631812: InProgress\n",
      "...\n",
      "2022-04-22 12:51:06 Starting - Preparing the instances for training......\n",
      "2022-04-22 12:52:07 Downloading - Downloading input data...\n",
      "2022-04-22 12:52:26 Training - Downloading the training image...\n",
      "2022-04-22 12:53:08 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-04-22 12:53:10,505 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-04-22 12:53:10,519 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-04-22 12:53:10,529 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-04-22 12:53:10,534 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-04-22 12:53:10,898 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting monai\n",
      "  Downloading monai-0.8.1-202202162213-py3-none-any.whl (721 kB)\u001b[0m\n",
      "\u001b[34mCollecting itk\n",
      "  Downloading itk-5.2.1.post1-cp36-cp36m-manylinux2014_x86_64.whl (8.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (7.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (0.25.0)\u001b[0m\n",
      "\u001b[34mCollecting itk-numerics==5.2.1.post1\n",
      "  Downloading itk_numerics-5.2.1.post1-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (54.5 MB)\u001b[0m\n",
      "\u001b[34mCollecting itk-io==5.2.1.post1\n",
      "  Downloading itk_io-5.2.1.post1-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\u001b[0m\n",
      "\u001b[34mCollecting itk-filtering==5.2.1.post1\n",
      "  Downloading itk_filtering-5.2.1.post1-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (95.3 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from itk->-r requirements.txt (line 2)) (1.16.4)\u001b[0m\n",
      "\u001b[34mCollecting itk-core==5.2.1.post1\n",
      "  Downloading itk_core-5.2.1.post1-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (70.6 MB)\u001b[0m\n",
      "\u001b[34mCollecting itk-segmentation==5.2.1.post1\n",
      "  Downloading itk_segmentation-5.2.1.post1-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.6 MB)\u001b[0m\n",
      "\u001b[34mCollecting itk-registration==5.2.1.post1\n",
      "  Downloading itk_registration-5.2.1.post1-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.3 MB)\u001b[0m\n",
      "\u001b[34mCollecting numpy\n",
      "  Downloading numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting torch>=1.6\n",
      "  Downloading torch-1.10.2-cp36-cp36m-manylinux1_x86_64.whl (881.9 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from torch>=1.6->monai->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch>=1.6->monai->-r requirements.txt (line 1)) (3.7.4.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 4)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 4)) (2021.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas->-r requirements.txt (line 4)) (1.15.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: numpy, itk-core, itk-numerics, itk-filtering, torch, itk-segmentation, itk-registration, itk-io, monai, itk\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.16.4\n",
      "    Uninstalling numpy-1.16.4:\n",
      "      Successfully uninstalled numpy-1.16.4\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.5.0\n",
      "    Uninstalling torch-1.5.0:\n",
      "      Successfully uninstalled torch-1.5.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed itk-5.2.1.post1 itk-core-5.2.1.post1 itk-filtering-5.2.1.post1 itk-io-5.2.1.post1 itk-numerics-5.2.1.post1 itk-registration-5.2.1.post1 itk-segmentation-5.2.1.post1 monai-0.8.1 numpy-1.19.5 torch-1.10.2\u001b[0m\n",
      "\u001b[34m2022-04-22 12:54:06,036 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-04-22 12:54:06,054 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-04-22 12:54:06,065 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-04-22 12:54:06,076 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"gloo\",\n",
      "        \"epochs\": 100\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2022-04-22-12-50-11-763\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-741261399688/pytorch-training-2022-04-22-12-50-11-763/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"monai_dicom_json\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"monai_dicom_json.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"backend\":\"gloo\",\"epochs\":100}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=monai_dicom_json.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=monai_dicom_json\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-741261399688/pytorch-training-2022-04-22-12-50-11-763/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"backend\":\"gloo\",\"epochs\":100},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-04-22-12-50-11-763\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-741261399688/pytorch-training-2022-04-22-12-50-11-763/source/sourcedir.tar.gz\",\"module_name\":\"monai_dicom_json\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"monai_dicom_json.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--epochs\",\"100\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=100\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 monai_dicom_json.py --backend gloo --epochs 100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mDistributed training - False\u001b[0m\n",
      "\u001b[34mNumber of gpus available - 0\u001b[0m\n",
      "\u001b[34mTraining count = 25\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 1/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 1.1249\u001b[0m\n",
      "\u001b[34mepoch 1 average loss: 1.1249\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 2/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 1.1110\u001b[0m\n",
      "\u001b[34mepoch 2 average loss: 1.1110\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 3/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 1.1044\u001b[0m\n",
      "\u001b[34mepoch 3 average loss: 1.1044\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 4/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 1.0829\u001b[0m\n",
      "\u001b[34mepoch 4 average loss: 1.0829\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 5/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 1.0807\u001b[0m\n",
      "\u001b[34mepoch 5 average loss: 1.0807\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 6/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 1.0573\u001b[0m\n",
      "\u001b[34mepoch 6 average loss: 1.0573\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 7/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 1.0369\u001b[0m\n",
      "\u001b[34mepoch 7 average loss: 1.0369\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 8/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 1.0292\u001b[0m\n",
      "\u001b[34mepoch 8 average loss: 1.0292\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 9/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 1.0359\u001b[0m\n",
      "\u001b[34mepoch 9 average loss: 1.0359\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 10/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 1.0112\u001b[0m\n",
      "\u001b[34mepoch 10 average loss: 1.0112\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 11/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 1.0004\u001b[0m\n",
      "\u001b[34mepoch 11 average loss: 1.0004\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 12/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.9949\u001b[0m\n",
      "\u001b[34mepoch 12 average loss: 0.9949\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 13/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.9942\u001b[0m\n",
      "\u001b[34mepoch 13 average loss: 0.9942\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 14/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.9689\u001b[0m\n",
      "\u001b[34mepoch 14 average loss: 0.9689\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 15/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.9780\u001b[0m\n",
      "\u001b[34mepoch 15 average loss: 0.9780\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 16/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.9535\u001b[0m\n",
      "\u001b[34mepoch 16 average loss: 0.9535\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 17/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.9213\u001b[0m\n",
      "\u001b[34mepoch 17 average loss: 0.9213\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 18/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.9188\u001b[0m\n",
      "\u001b[34mepoch 18 average loss: 0.9188\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 19/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.9395\u001b[0m\n",
      "\u001b[34mepoch 19 average loss: 0.9395\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 20/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.9087\u001b[0m\n",
      "\u001b[34mepoch 20 average loss: 0.9087\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 21/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.9252\u001b[0m\n",
      "\u001b[34mepoch 21 average loss: 0.9252\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 22/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.8989\u001b[0m\n",
      "\u001b[34mepoch 22 average loss: 0.8989\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 23/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.9044\u001b[0m\n",
      "\u001b[34mepoch 23 average loss: 0.9044\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 24/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.8855\u001b[0m\n",
      "\u001b[34mepoch 24 average loss: 0.8855\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 25/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.9102\u001b[0m\n",
      "\u001b[34mepoch 25 average loss: 0.9102\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 26/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.8567\u001b[0m\n",
      "\u001b[34mepoch 26 average loss: 0.8567\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 27/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.8647\u001b[0m\n",
      "\u001b[34mepoch 27 average loss: 0.8647\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 28/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.8369\u001b[0m\n",
      "\u001b[34mepoch 28 average loss: 0.8369\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 29/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.7986\u001b[0m\n",
      "\u001b[34mepoch 29 average loss: 0.7986\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 30/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.6015\u001b[0m\n",
      "\u001b[34mepoch 66 average loss: 0.6015\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 67/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.6217\u001b[0m\n",
      "\u001b[34mepoch 67 average loss: 0.6217\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 68/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.6083\u001b[0m\n",
      "\u001b[34mepoch 68 average loss: 0.6083\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 69/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.5143\u001b[0m\n",
      "\u001b[34mepoch 69 average loss: 0.5143\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 70/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m1/0, train_loss: 0.5861\u001b[0m\n",
      "\u001b[34mepoch 70 average loss: 0.5861\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 71/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.6204\u001b[0m\n",
      "\u001b[34mepoch 71 average loss: 0.6204\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 72/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.5834\u001b[0m\n",
      "\u001b[34mepoch 72 average loss: 0.5834\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 73/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.6370\u001b[0m\n",
      "\u001b[34mepoch 73 average loss: 0.6370\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 74/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.5253\u001b[0m\n",
      "\u001b[34mepoch 74 average loss: 0.5253\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 75/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.5795\u001b[0m\n",
      "\u001b[34mepoch 75 average loss: 0.5795\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 76/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.5324\u001b[0m\n",
      "\u001b[34mepoch 76 average loss: 0.5324\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 77/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.6173\u001b[0m\n",
      "\u001b[34mepoch 77 average loss: 0.6173\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 78/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.6270\u001b[0m\n",
      "\u001b[34mepoch 78 average loss: 0.6270\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 79/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.5087\u001b[0m\n",
      "\u001b[34mepoch 79 average loss: 0.5087\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 80/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.5349\u001b[0m\n",
      "\u001b[34mepoch 80 average loss: 0.5349\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 81/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.5759\u001b[0m\n",
      "\u001b[34mepoch 81 average loss: 0.5759\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 82/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.5850\u001b[0m\n",
      "\u001b[34mepoch 82 average loss: 0.5850\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 83/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.5151\u001b[0m\n",
      "\u001b[34mepoch 83 average loss: 0.5151\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 84/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.4820\u001b[0m\n",
      "\u001b[34mepoch 84 average loss: 0.4820\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 85/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.5926\u001b[0m\n",
      "\u001b[34mepoch 85 average loss: 0.5926\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 86/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.6563\u001b[0m\n",
      "\u001b[34mepoch 86 average loss: 0.6563\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 87/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.4943\u001b[0m\n",
      "\u001b[34mepoch 87 average loss: 0.4943\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 88/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.5127\u001b[0m\n",
      "\u001b[34mepoch 88 average loss: 0.5127\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 89/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.5079\u001b[0m\n",
      "\u001b[34mepoch 89 average loss: 0.5079\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 90/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.5261\u001b[0m\n",
      "\u001b[34mepoch 90 average loss: 0.5261\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 91/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.4642\u001b[0m\n",
      "\u001b[34mepoch 91 average loss: 0.4642\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 92/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.5741\u001b[0m\n",
      "\u001b[34mepoch 92 average loss: 0.5741\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 93/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.4610\u001b[0m\n",
      "\u001b[34mepoch 93 average loss: 0.4610\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 94/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.4821\u001b[0m\n",
      "\u001b[34mepoch 94 average loss: 0.4821\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 95/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.5956\u001b[0m\n",
      "\u001b[34mepoch 95 average loss: 0.5956\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 96/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.4532\u001b[0m\n",
      "\u001b[34mepoch 96 average loss: 0.4532\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 97/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.4214\u001b[0m\n",
      "\u001b[34mepoch 97 average loss: 0.4214\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 98/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.4557\u001b[0m\n",
      "\u001b[34mepoch 98 average loss: 0.4557\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 99/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.4166\u001b[0m\n",
      "\u001b[34mepoch 99 average loss: 0.4166\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mepoch 100/100\u001b[0m\n",
      "\u001b[34minputs shape is ----- torch.Size([25, 512, 512, 1])\u001b[0m\n",
      "\u001b[34minputs shape after is ----- torch.Size([25, 1, 512, 512])\u001b[0m\n",
      "\u001b[34m1/0, train_loss: 0.4175\u001b[0m\n",
      "\u001b[34mepoch 100 average loss: 0.4175\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34m2022-04-22 13:29:03,050 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-04-22 13:29:19 Uploading - Uploading generated training model\n",
      "2022-04-22 13:29:35 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "estimator.fit({'train': inputs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the endpoint \n",
    "\n",
    "+ default inference with `numpy` as input\n",
    "\n",
    "+ customized inference with `JSON` file pointing to the image file in S3 [./source/inference.py]\n",
    "\n",
    "for further information, you may refer to [pytoch-inference-hander](https://github.com/aws/sagemaker-pytorch-inference-toolkit/blob/master/src/sagemaker_pytorch_serving_container/default_pytorch_inference_handler.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "## Option 0: default\n",
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data=estimator.__dict__['output_path']+estimator.__dict__['_current_job_name']+'/output/model.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Option 1: BYOS\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "\n",
    "model = PyTorchModel(\n",
    "    entry_point=\"inference.py\", ## inference code with customerization\n",
    "    source_dir=\"code\",        ## folder with the inference code\n",
    "    role=role,\n",
    "    model_data=model_data,\n",
    "    framework_version=\"1.5.0\",\n",
    "    py_version=\"py3\",\n",
    ")\n",
    "predictor2 = model.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge',entry_point='inference.py',source_dir='code',\n",
    "                            serializer=sagemaker.serializers.JSONSerializer(),deserializer=sagemaker.deserializers.JSONDeserializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with deployed endpoints\n",
    "\n",
    "+ use `Numpy` dataset as input for predictor\n",
    "\n",
    "+ use `JSON` file as input for predictor2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: using Default handler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['normal', 'cap', 'covid', 'cap']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_file_list\n",
    "inf_test_label=[x.split('-')[0] for x in image_file_list]\n",
    "inf_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code.train import DICOMDataset\n",
    "import torch\n",
    "\n",
    "def get_val_data_loader(valX, valY):\n",
    "    val_transforms = Compose([\n",
    "    LoadImage(image_only=True),\n",
    "    ScaleIntensity(),\n",
    "    Resize(spatial_size=(512,-1)),\n",
    "    ToTensor()\n",
    "    ])\n",
    "    \n",
    "    dataset = DICOMDataset(valX, valY, val_transforms)\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=1, num_workers=1)\n",
    "\n",
    "val_loader = get_val_data_loader(image_file_list, inf_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Transform input info -- LoadImage ===\n",
      "\n",
      "=== Transform input info -- LoadImage ===\n",
      "Data statistics:\n",
      "Type: <class 'str'> None\n",
      "Value: 01b0366f-11e3-74ee-60aa-0d3098fc4743-IM0022.dcm\n",
      "Data statistics:\n",
      "Type: <class 'str'> None\n",
      "Value: 01b0366f-11e3-74ee-60aa-0d3098fc4743-IM0022.dcm\n",
      "\n",
      "=== Transform input info -- LoadImage ===\n",
      "\n",
      "=== Transform input info -- LoadImage ===\n",
      "\n",
      "=== Transform input info -- LoadImage ===\n",
      "Data statistics:\n",
      "Type: <class 'str'> None\n",
      "Value: f5e77d4d-cf30-cf2d-9097-a70286ee6241-IM0043.dcm\n",
      "Data statistics:\n",
      "Type: <class 'str'> None\n",
      "Value: f5e77d4d-cf30-cf2d-9097-a70286ee6241-IM0043.dcm\n",
      "Data statistics:\n",
      "Type: <class 'str'> None\n",
      "Value: f5e77d4d-cf30-cf2d-9097-a70286ee6241-IM0043.dcm\n",
      "\n",
      "=== Transform input info -- LoadImage ===\n",
      "\n",
      "=== Transform input info -- LoadImage ===\n",
      "\n",
      "=== Transform input info -- LoadImage ===\n",
      "\n",
      "=== Transform input info -- LoadImage ===\n",
      "Data statistics:\n",
      "Type: <class 'str'> None\n",
      "Value: 573e8d77-550a-e889-8ff4-1e8d8944897c-IM0106.dcm\n",
      "Data statistics:\n",
      "Type: <class 'str'> None\n",
      "Value: 573e8d77-550a-e889-8ff4-1e8d8944897c-IM0106.dcm\n",
      "Data statistics:\n",
      "Type: <class 'str'> None\n",
      "Value: 573e8d77-550a-e889-8ff4-1e8d8944897c-IM0106.dcm\n",
      "Data statistics:\n",
      "Type: <class 'str'> None\n",
      "Value: 573e8d77-550a-e889-8ff4-1e8d8944897c-IM0106.dcm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/monai/transforms/transform.py\", line 82, in apply_transform\n    return _apply_transform(transform, data, unpack_items)\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/monai/transforms/transform.py\", line 53, in _apply_transform\n    return transform(parameters)\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/monai/transforms/io/array.py\", line 194, in __call__\n    img = reader.read(filename)\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/monai/data/image_reader.py\", line 217, in read\n    img_.append(itk.imread(name, **kwargs_))\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/itk/support/extras.py\", line 965, in imread\n    reader = template_reader_type.New(**kwargs)\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/itk/support/template_class.py\", line 661, in New\n    itk.ImageFileReader, False, \"FileName\", *args, **kwargs\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/itk/support/template_class.py\", line 161, in _NewImageReader\n    f\"Could not create IO object for reading file {inputFileName}\" + msg\nRuntimeError: Could not create IO object for reading file 01b0366f-11e3-74ee-60aa-0d3098fc4743-IM0022.dcm\nThe file doesn't exist. \nFilename = 01b0366f-11e3-74ee-60aa-0d3098fc4743-IM0022.dcm\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ec2-user/SageMaker/MONAI-MedicalImage-SageMaker/Classification/code/train.py\", line 42, in __getitem__\n    return self.transforms(self.image_files[index]), self.labels[index]\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/monai/transforms/compose.py\", line 160, in __call__\n    input_ = apply_transform(_transform, input_, self.map_items, self.unpack_items)\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/monai/transforms/transform.py\", line 106, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <monai.transforms.io.array.LoadImage object at 0x7f27b4ea6908>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-2c69855a4c96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;34m\"Normal\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Cap\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Covid\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/monai/transforms/transform.py\", line 82, in apply_transform\n    return _apply_transform(transform, data, unpack_items)\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/monai/transforms/transform.py\", line 53, in _apply_transform\n    return transform(parameters)\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/monai/transforms/io/array.py\", line 194, in __call__\n    img = reader.read(filename)\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/monai/data/image_reader.py\", line 217, in read\n    img_.append(itk.imread(name, **kwargs_))\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/itk/support/extras.py\", line 965, in imread\n    reader = template_reader_type.New(**kwargs)\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/itk/support/template_class.py\", line 661, in New\n    itk.ImageFileReader, False, \"FileName\", *args, **kwargs\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/itk/support/template_class.py\", line 161, in _NewImageReader\n    f\"Could not create IO object for reading file {inputFileName}\" + msg\nRuntimeError: Could not create IO object for reading file 01b0366f-11e3-74ee-60aa-0d3098fc4743-IM0022.dcm\nThe file doesn't exist. \nFilename = 01b0366f-11e3-74ee-60aa-0d3098fc4743-IM0022.dcm\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ec2-user/SageMaker/MONAI-MedicalImage-SageMaker/Classification/code/train.py\", line 42, in __getitem__\n    return self.transforms(self.image_files[index]), self.labels[index]\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/monai/transforms/compose.py\", line 160, in __call__\n    input_ = apply_transform(_transform, input_, self.map_items, self.unpack_items)\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/monai/transforms/transform.py\", line 106, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <monai.transforms.io.array.LoadImage object at 0x7f27b4ea6908>\n"
     ]
    }
   ],
   "source": [
    "class_names = [ \"Normal\",\"Cap\", \"Covid\",]\n",
    "for i, val_data in enumerate(val_loader):\n",
    "    inputs = val_data[0].permute(0,3, 2, 1)\n",
    "    print(inputs)\n",
    "    response = predictor.predict(inputs)\n",
    "    pred = torch.nn.functional.softmax(torch.tensor(response), dim=1)\n",
    "    top_p, top_class = torch.topk(pred, 1)\n",
    "    actual_label = val_data[1]\n",
    "    print('actual class is ', val_data[1])\n",
    "    \n",
    "    print(\"predicted probability: \", pred)\n",
    "    print('predicted class: '+class_names[top_class])\n",
    "    print('predicted class probablity: '+str(round(top_p.item(),2)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: using handler with image input in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_data'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadir_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "key='test_data'\n",
    "#S3Uploader.upload('./data', path) \n",
    "\n",
    "\n",
    "## IF UPLOAD THE DATA TO S3, DO THE FOLLOWING STEP. we may skip the step if the the data has already been uploaded\n",
    "test_input = sess.upload_data(path=datadir_test, bucket=bucket,key_prefix=key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "sess =sagemaker.Session()\n",
    "predictor2=Predictor(endpoint_name='pytorch-inference-2022-04-22-13-30-08-191', sagemaker_session=sess, serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer())\n",
    "                    # serializer=<sagemaker.serializers.IdentitySerializer object>, deserializer=<sagemaker.deserializers.BytesDeserializer object>, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-us-east-1-741261399688'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.48 ms, sys: 333 µs, total: 4.81 ms\n",
      "Wall time: 2.67 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'results': {'class': 'Normal', 'probability': 0.89}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "payload={\"bucket\": bucket,\n",
    "    \"key\":\"test_data/normal-IM0062.dcm\"}\n",
    "\n",
    "predictor2.predict(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
